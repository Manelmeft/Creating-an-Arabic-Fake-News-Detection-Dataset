{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c978a1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les titres et les URLs des articles ont été enregistrés dans le fichier articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "url = \"https://misbar.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "html_content = driver.page_source\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "article_data = []\n",
    "\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if '/editorial/' in link['href'] or '/factcheck/' in link['href']:\n",
    "        article_url = \"https://misbar.com/\" + link['href']\n",
    "        \n",
    "        \n",
    "        article_title = link.text.strip()\n",
    "        \n",
    "        article_data.append([article_title, article_url])\n",
    "\n",
    "with open('articles.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Topic', 'URL', 'type'])  \n",
    "    csv_writer.writerows(article_data)\n",
    "\n",
    "print(\"Les titres et les URLs des articles ont été enregistrés dans le fichier articles.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bfd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction des articles à partir de http://norumors.net/\n",
      "\n",
      "Les titres et les URLs des articles ont été ajoutés dans le fichier articles.csv.\n",
      "Extraction des articles à partir de https://fatabyyano.net/\n",
      "\n",
      "Les titres et les URLs des articles ont été ajoutés dans le fichier articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"http://norumors.net/\",\n",
    "    #\"https://verify-sy.com/\",\n",
    "    \"https://fatabyyano.net/\"\n",
    "]\n",
    "\n",
    "def extract_articles(url):\n",
    "    articles_data = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_titles = soup.find_all('h2')\n",
    "        for title in article_titles:\n",
    "            article_title = title.text.strip()  \n",
    "            article_url = title.find_previous('a', href=True)\n",
    "            if article_url:\n",
    "                article_url = article_url['href']\n",
    "            else:\n",
    "                article_url = \"Non disponible\"\n",
    "            articles_data.append([article_title, article_url])  \n",
    "    \n",
    "    with open('articles.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        if csvfile.tell() == 0:  \n",
    "        csv_writer.writerows(articles_data)\n",
    "\n",
    "    print(\"Les titres et les URLs des articles ont été ajoutés dans le fichier articles.csv.\")\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Extraction des articles à partir de {url}\\n\")\n",
    "    extract_articles(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b11d5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toutes les données ont été enregistrées dans le fichier articles.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "base_url1 = \"https://verify-sy.com/list/112/%D9%85%D8%A4%D9%83%D8%AF?page={}\"\n",
    "base_url2 = \"https://verify-sy.com/list/16/%D8%A7%D8%AD%D8%AA%D9%8A%D8%A7%D9%84?page={}\"\n",
    "\n",
    "\n",
    "total_pages = 3\n",
    "\n",
    "\n",
    "all_articles_data = []\n",
    "\n",
    "\n",
    "def extract_articles(url, article_type):\n",
    "    articles_data = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        h3_tags = soup.find_all('h3', class_='list-title-ca')\n",
    "        for h3_tag in h3_tags:\n",
    "            link = h3_tag.find('a')\n",
    "            if link:\n",
    "                article_url = link['href']\n",
    "                article_title = link.text.strip()\n",
    "                articles_data.append([article_title, article_url, article_type])\n",
    "    return articles_data\n",
    "\n",
    "\n",
    "for page_number in range(1, total_pages + 1):\n",
    "    url = base_url1.format(page_number)\n",
    "    all_articles_data.extend(extract_articles(url, 'reel'))\n",
    "\n",
    "\n",
    "for page_number in range(1, total_pages + 1):\n",
    "    url = base_url2.format(page_number)\n",
    "    all_articles_data.extend(extract_articles(url, 'fake'))\n",
    "\n",
    "\n",
    "with open('articles.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    csv_writer.writerows(all_articles_data)\n",
    "\n",
    "print(\"Toutes les données ont été enregistrées dans le fichier articles.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "563f06d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les articles ont été catégorisés et enregistrés \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "categories_arabic = {\n",
    "    'Politics': ['الجيش','وزارة','حروب','لاجئة','قتلت','الدولي','الأمة','سياسة', 'انتخابات', 'حكومة', 'برلمان','الدولة', 'ساسة', 'نظام سياسي', 'ثورة', 'ديمقراطية', 'حرب', 'دبلوماسية'],\n",
    "    'Religion': ['السلامة','الله','دين', 'إيمان', 'روحانية', 'معبد', 'الكنيسة', 'ديانة', 'مسيحي', 'إسلامي', 'صلاة', 'قرآن', 'رمضان', 'صوم'],\n",
    "    'Health': ['دماء','السرطان','جنس','طبي','صحة', 'مرض', 'طب', 'تغذية', 'وباء', 'صحي', 'علاج', 'مستشفى', 'لقاح', 'نظام غذائي', ' صحي', ' طبية'],\n",
    "    'Technology': ['تكنولوجيا', 'إنترنت', 'تطبيق', 'رقمي', 'ذكاء اصطناعي', 'تقنية', 'تطوير', 'برمجة', 'شبكات اجتماعية', 'تشفير', 'هواتف ذكية', 'تطبيقات محمولة'],\n",
    "    'Sports': ['رياضة', 'كرة القدم', 'تنس', 'سباحة', 'ألعاب أولمبية', 'مباراة', 'فريق', 'تدريب', 'ماراثون', 'جمباز', 'محترفون', ' رياضية'],\n",
    "    'Business': ['أعمال', 'تجارة', 'اقتصاد', 'تطوير الأعمال', ' المال', 'شركة', 'استثمار', 'ريادة الأعمال', 'تسويق', 'ربحية', 'إدارة أعمال', ' توريد'],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "def categorize_article(title):\n",
    "    category = None\n",
    "    \n",
    "    for category_arabic, keywords in categories_arabic.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in title:\n",
    "                category = category_arabic\n",
    "                break\n",
    "    \n",
    "    if category is None:\n",
    "        category = 'Other'  \n",
    "                \n",
    "    return category\n",
    "\n",
    "\n",
    "categorized_articles = []\n",
    "with open('articles.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader)  # Ignorer l'en-tête\n",
    "    for row in csv_reader:\n",
    "        title = row[0] if len(row) >= 1 else ''  \n",
    "        url = row[1] if len(row) >= 2 else ''  \n",
    "        article_type = row[2] if len(row) >= 3 else ''  \n",
    "        category = categorize_article(title)\n",
    "        categorized_articles.append([title, url, article_type, category])\n",
    "\n",
    "        if article_type == '':\n",
    "            categorized_articles.append([title, url, article_type, category])\n",
    "\n",
    "\n",
    "with open('Data_Collection.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Title', 'URL', 'Type', 'Category'])  \n",
    "    csv_writer.writerows(categorized_articles)\n",
    "\n",
    "print(\"Les articles ont été catégorisés et enregistrés \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11cbbdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Religion: 13\n",
      "Politics: 25\n",
      "Health: 19\n",
      "Other: 120\n",
      "Sports: 4\n",
      "Business: 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize a counter to store the count of each category\n",
    "category_counter = Counter()\n",
    "\n",
    "# Iterate through the categorized articles and count occurrences of each category\n",
    "for article in categorized_articles:\n",
    "    category = article[3]  # The category is stored at index 3\n",
    "    category_counter[category] += 1\n",
    "\n",
    "# Print the count of articles in each category\n",
    "for category, count in category_counter.items():\n",
    "    print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd816d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
